```{r create_basic_documentation, echo=TRUE}
# Before anything else, set global options
opts_chunk$set(echo=TRUE, cache=FALSE, error=TRUE)

doc <- NULL
doc$run.date <- date()
doc$version <- system(' git rev-parse HEAD', intern=TRUE)
doc$author <- "Steve Simon (KUMC)"
doc$maintainer <- "Steve Simon (KUMC)"
doc$assistants <- "Dan Connolly"
```


Case-control
============
For context, see [485].

[485]: https://informatics.gpcnetwork.org/trac/Project/ticket/485
[bc_qa]: https://bitbucket.org/gpcnetwork/bc_qa

This program reads data from PCRONET CDM and matches it
with patient numbers from i2b2 to get a case-control
data set. It stores the data and some intermediate files in
an RData file.

Here's a summary of how the data was selected. We
identified four disease groups in Heron (KUMC's i2b2).

* Breast cancer (as identified in the SEER Site Summary),

* All other cancers (again from SEER),

* Diabetes (ICD9 250 or ICD10 E08-E13), and

* Ischemic heart disease (ICD9 410-414.99 or ICD10 I20-I25).

All males were excluded.

The first group represents cases and the other three
represent different control groups. Patients who qualified
for two or more of the above groups were excluded from the
analysis.

There were no date restrictions. In future versions, we may want
to consider some date restrictions and try matching subjects on
age.

We then merged that data with the PCORNET CDM to get 
any ICD9 codes associated with each patient.

Two companion programs, analyze_lasso.Rmd and analyze_sensitivity.Rmd,
take the resulting data set and produces simple graphs and analyses. 

This program was run on `r doc$date` using version `r doc$version`.
The original author is `r doc$author`. `r doc$maintainer`
is currently maintaining and enhancing this program
with the assistance of `r doc$assistants`.


```{r run_preliminaries}

# Backup image (just in case) and then start with a blank slate.

save.image("backup.RData")
rm(list=ls())

# Set qc option, if TRUE, print various intermediate values and quality checks.
# If drop_intermediate_objects is TRUE, then intermediate values are removed
# once they are no longer needed. You might keep the intermediate values
# around if you are debugging.

qc <- TRUE
drop_intermediate_objects <- TRUE

# Document when this program started.

start_time <- Sys.time()
if (qc) {
  cat("\n\nProgram started at ")
  print(start_time)
}

# clean out the old archive

rm(arc)

# load required libraries.

library("ROracle")
library("RSQLite")
library("knitr")

# Check to see if you are in the proper subdirectory.

if (qc) {
  cat("\n\nQuality check: Are we in the correct directory?\n")
  print(getwd())
}

# Control wrapping

options(width=90)

# Here are some functions needed in this program.

source("create_utility_functions.R")

```

Set up storage for temporary data frames and matrices (d)
and notes for my own reference.

Design the appropriate sql queries.

```{r design_sql_queries, timer=TRUE}

# I need to put all my SQL queries in one spot so
# I can model one SQL query after the previous one.
# As I get better with SQL, I will move the queries
# closer to the program location where they get used.

sql <- NULL

sql[["observation_fact"]] <-
  "select o.patient_num, o.concept_cd c.name_char 
   from observation_fact o
   inner join (concept_dimension c
     where concept_path LIKE '\\i2b2\\Procorders%' OR
       concept_path LIKE '\\i2b2\\Procedures%')
   ON o.concept_cd=c.concept_cd"

if (qc) {
  for (i in 1:length(sql)) {
    print(names(sql)[i])
    cat(sql[[i]])
    cat("\n\n")
  }
}

```

This section reads in the patient ids for the various disease groups.

The file, disease_group_databases.txt, should have one entry for each file.
It also has information about which site the data 
comes from and which disease group the patients come from.

```{r get_patient_database_names, timer=TRUE}

di <- "/d1/home/ssimon/bc_qa/bc_case_control"
setwd(di)
fn <- "disease_group_databases.txt"
pa <- read.csv(file=fn, header=TRUE, stringsAsFactors=FALSE)

i_case <- pa$disease_group[pa$case==1]
i_control <- pa$disease_group[pa$case==0]

if (qc) print_random_rows(pa)
archive(pa,"patient file information")
```

Now loop across file names to get patient numbers for each group.

```{r get_patient_numbers, timer=TRUE}

# pb: distinct patient numbers in each disease group

# Eventually, I will need to pull dates from this file. 
# Look for start_date in the observation_fact table.

sql[["distinct_patients"]] <-
  "select distinct patient_num
   from observation_fact"

pb <- NULL
for (i in 1:length(pa$file_location)) {
  p_connect <- dbConnect(SQLite(), dbname=pa$file_location[i])
  group_name <- pa$disease_group[i] 
  pb[[group_name]] <- dbGetQuery(p_connect, sql["distinct_patients"])
  dbDisconnect(p_connect)
}

if (qc) {
  for (i in 1:length(pb)) {
    print(names(pb)[i])
    print(dim(pb[[i]]))
    print_random_rows(pb[[i]])
  }
}

pc <- data.frame(gp=names(pb[1]), pb[[1]], stringsAsFactors=FALSE)
for (k in 2:length(pb)) {
  pc <- rbind(pc, data.frame(gp=names(pb[k]), pb[[k]], stringsAsFactors=FALSE))
}
archive(pc, "stacked patient numbers")
if (drop_intermediate_objects) rm(pa)
if (drop_intermediate_objects) rm(pb)
if (qc) {
  print_random_rows(pc)
}
```

Next, you need to write the patient numbers to the same
location as the PCORnet CDM database.

```{r write_patient_numbers_to_cdm, timer=TRUE}

# This code borrowed from cdm_fun.Rmd.

cdm_config <- read.csv('../cdm_config.csv', stringsAsFactors=FALSE)
missing_config <- setdiff(c('account', 'password'), names(cdm_config))
stopifnot(length(missing_config) == 0)
c_connect <-
  dbConnect(Oracle(), cdm_config$account, cdm_config$password, cdm_config$access)

if (qc) {
  cat("Simple test")
  dbGetQuery(c_connect, "select * from pcornet_cdm.diagnosis where rownum < 10")
}

names(pc) <- toupper(names(pc))
dbWriteTable(c_connect, "PC", pc, overwrite=TRUE)

if (qc) dbListTables(c_connect)

```

Now pull out all the diagnosis codes associated with our patient list.

```{r extract_cdm_data, timer=TRUE}

# Note: some patients have no ICD9 codes. That may be okay.

sql[["matching_count"]] <- 
  "select distinct j.PATIENT_NUM, j.GP, p.PATID
   from pcornet_cdm.diagnosis P
   JOIN PC j
   on p.PATID = j.PATIENT_NUM
   where p.DX_TYPE = '09'"

if (qc) {
  mc <- dbGetQuery(c_connect, sql[["matching_count"]])
  count_unique_patients(mc, pc)
  print(table(mc$GP))
  print(table(pc$GP))
} 

sql[["patient_diagnoses"]] <-
  "select PATIENT_NUM, GP, DX, ADMIT_DATE as observation_date
   from pcornet_cdm.diagnosis
   join PC
   on PATID = PATIENT_NUM
   where DX_TYPE='09'"

ca <- dbGetQuery(c_connect, sql[["patient_diagnoses"]])
archive(ca, "Query from pcornet_cdm")
```


Now, let's allow ourselves the ability to screen out
any diagnosis codes that occur infrequently among the
cases.

```{r find_common_dx, timer=TRUE}

# Add a dx_count column that notes how often the DX appears
# among the case groups. This will allow you to select only
# the "popular" DX codes.

cb <- table(ca$DX[ca$GP==i_case])
archive(cb, "CDM diagnosis counts")

# Note: as.numeric is needed here because you need to strip out
# the names before merging.
# Also note that this merge statement will remove any diagnosis
# codes which occur only among the controls.

cc <- data.frame(DX=names(cb), dx_count=as.numeric(cb))
archive(cc, "CDM counts data.frame")

cd <- merge(ca, cc)
cd$dx_new <- align_icd9_codes(cd$DX)

if (qc) {
  print_random_rows(cc)
  print_random_rows(cd)
  print(length(unique(ca$DX)))
  print(length(unique(cd$DX)))
}
archive(cd, "Diagnoses merged with counts")
if (drop_intermediate_objects) rm(ca)
if (drop_intermediate_objects) rm(cb)
if (drop_intermediate_objects) rm(cc)

```

While you can get the nice labels for icd9 codes from i2b2, it is easier
just to pull them from another source. I chose labels from the
[https://www.cms.gov/medicare/coding/ICD9providerdiagnosticcodes/codes.html CMS]
site. 

```{r get_nice_names, timer=TRUE}

fn <- "icd9_labels.csv"
ce <- read.csv(fn, header=TRUE, as.is=TRUE, row.names=NULL)

ce$dx_new <- align_icd9_codes(ce$diagnosis_code)
ce$nice_label <- paste(strip_specials(ce$short_label),ce$DX,sep="_")
ce$dx_label <- paste(strip_specials(ce$short_label), ce$dx_new, sep="_")
archive(ce, "ICD9 codes and labels")

if (qc) print_random_rows(ce$nice_label)

# Here are diagnosis codes in cd, not found in ce.
if (qc) {
  print_random_rows(sort(setdiff(cd$DX,ce$DX)))
  print_random_rows(sort(setdiff(cd$dx_new,ce$dx_new)))
}

cf <- merge(cd, ce[, c("dx_new","dx_label")], all.x=TRUE, all.y=FALSE)
archive(cf, "CDM with nice labels")
if (drop_intermediate_objects) rm(cd)
if (drop_intermediate_objects) rm(ce)

if (qc) print_random_rows(cf)

# Before we leave this section, save the DX and dx_label
# values to allow later re-merges.

cg <- cf[!duplicated(cf$DX), c("DX", "dx_label")]
                               ```
if (qc) print_random_rows(cg)
archive(cg, "CDM code key")
if (drop_intermediate_objects) rm(cf)
```

In addition to CDM data, you can get lots of fun stuff
from i2b2 queries. The individual i2b2 queries appear
in i2b2_variables_databases.txt.

```{r now_get_i2b2_codes, timer=TRUE}

# I need to get dates from here. Look for start_time in
# the observation_fact table.

fn <- "i2b2_variables_databases.txt"
ia <- read.csv(file=fn, header=TRUE, stringsAsFactors=FALSE)

archive(ia, "i2b2 file information")
if (qc) print(ia)

for (f in ia$file_location) {
  variables_conn <- dbConnect(SQLite(), dbname=f)
  dbWriteTable(variables_conn, "PC", pc, overwrite=TRUE)
  if (qc) dbListTables(variables_conn)
}

of <- NULL

# Note where clause in the middle of this SQL code.
# I2B2 carriers along a lot of extra stuff when you
# export and this is an attempt to strip down to
# only the things you really need. 
#
# Unfortunately, it is specific to the particular
# i2b2 sources that you are using.

sql[["observation_fact"]] <-
  "select patient_num, GP, concept_cd, name_char, start_date AS observation_date 
   from observation_fact
   inner join (
     select concept_cd AS ccd, name_char
     from concept_dimension
     where
       concept_path LIKE '\\i2b2\\Procorders%' OR
       concept_path LIKE '\\i2b2\\Procedures%'
   )
   on concept_cd=ccd
   inner join (
     select PATIENT_NUM AS pn, GP
     from PC
   )
   on patient_num = pn"


for (f in ia$file_location) {
  variables_conn <- dbConnect(SQLite(), dbname=f)
  of[[f]] <- dbGetQuery(variables_conn, sql[["observation_fact"]])
}

# Since we are importing multiple i2b2 files, we need to stack these
# into a single data frame.

ib <- of[[1]]
for (i in 2:length(of)) {
  ib <- rbind(ib, of[[i]])
}

if (qc) {
  print(dim(ib))
  print_random_rows(ib)
  count_unique_patients(ib, pc)
}
archive(ib, "Stacked i2b2 data")
if (drop_intermediate_objects) rm(ia)

# Add a concept_count column that notes how often concept_cd appears
# among the case groups. This will allow you to select only
# the "popular" concept_cd codes.

ic <- table(ib$concept_cd[ib$GP==i_case])

if (qc) print_random_rows(ic)
archive(ic, "Counts among cases")

# Note: as.numeric is needed here because you need to strip out
# the names before merging.

# Also note that this merge statement will remove any diagnosis
# codes which occur only among the controls.

id <- data.frame(concept_cd=names(ic), dx_count=as.numeric(ic))

if (qc) print_random_rows(id)
archive(id, "Convert to data.frame")
if (drop_intermediate_objects) rm(ic)

ie <- merge(ib, id)
ie$name_char <- strip_specials(ie$name_char)
ie$dx_label <- paste(ie$name_char, ie$concept_cd, sep="_")

if (qc) print_random_rows(ie)
archive(ie, "Merge counts back in")
if (drop_intermediate_objects) rm(ib)
if (drop_intermediate_objects) rm(id)

i_names <- c("patient_num", "GP", "dx_count", "dx_label", "observation_date")
c_names <- c("PATIENT_NUM", "GP", "dx_count", "dx_label", "OBSERVATION_DATE")
iz <- ie[, i_names]
cz <- cf[, c_names]
if (drop_intermediate_objects) rm(cf)

names(iz) <- tolower(names(iz))
names(cz) <- tolower(names(cz))
                
lb <- rbind(cz, iz)
# make the labels look a bit nicer.
lb$dx_label <- tolower(lb$dx_label)
remove_list <-
  c("_icd9", "_kuh", "|proc_id", 
    "_at_", "_and_", "_for_",
    "_from_", 
    "_in_", "_into_", "_of_",
    "_or_", "_on_", "_to_", 
    "_use_", "_using_", "_with_") 
for (i in remove_list) {
  lb$dx_label <- sub(i, "_", lb$dx_label, fixed=TRUE)
}
lb$dx_label <- gsub(":", "_", lb$dx_label, fixed=TRUE)
lb$dx_label <- gsub("__", "_", lb$dx_label, fixed=TRUE)
lb$dx_label <- gsub("__", "_", lb$dx_label, fixed=TRUE)

st <- pc

if (qc) {
  cat("How many patients in lb come from cz?\n")
  count_unique_patients(cz, lb)
  cat("How many patients in lb come from iz?\n")
  count_unique_patients(iz, lb)
  cat("How many patients are in lb versus pc?\n")
  count_unique_patients(lb, pc)
}

# Before we go, save the concept_cd and dx_label values
# so that we can re-merge some files later.

ig <- ie[!duplicated(ie$concept_cd), c("concept_cd", "dx_label")]
archive(ig, "I2B2 code key")
if (drop_intermediate_objects) rm(ie)
if (qc) print_random_rows(ig)
names(cg) <- c("original_code", "dx_label")
names(ig) <- c("original_code", "dx_label")
zg <- rbind(cg, ig)
archive(zg, "Combined code key")
write.csv(zg, file="code_key.csv", row.names=FALSE)
if (qc) print_random_rows(zg)

```

Now save everything for later use.

```{r save-everything, timer=TRUE}
# extact_case_control.RData stores everything, including intermediate files.
# case_control_data.RData stores only the important stuff.

save.image(file="extract_case_control.RData")
save(
  lb, st, i_case, i_control,
  file="case_control_data.RData")
```

Well done. Here is how long everything took.

```{r display_timing_log}
if (qc) {
  cat("Program began at ")
  cat(as.character(start_time))
  cat("\nProgram ended at ")
  cat(as.character(Sys.time()))
  cat("\n\n")
  tm <- read.table("timing_log.txt")$V1
  cat(paste(tm, collapse="\n"))
}
```
