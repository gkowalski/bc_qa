```{r create_basic_documentation, echo=TRUE}

# This is a test change made at 12:37pm on Wednesday, June 22.
# This is a second change made at 12:43pm, same day.
# This is a third change made at 1:03pm, same day.

# Before anything else, set global options
opts_chunk$set(echo=TRUE, cache=FALSE, error=TRUE)

doc <- NULL
doc$run.date <- date()
doc$version <- system(' git rev-parse HEAD', intern=TRUE)
doc$author <- "Steve Simon (KUMC)"
doc$maintainer <- "Steve Simon (KUMC)"
doc$assistants <- "Dan Connolly"
```

First things first. If you run any of these programs, they 
will near the start use a command rm(list=ls()) that cleans
out any files left from previous programs. This helps improve
the reproducibility of the programs. But if you are in the 
habit of storing information in the default location,
.RData, this program may accidentally wipe out your stuff.

I've included a save.image("backup.Rdata") command but that
only provides a fig leaf of protection. You should run these
programs in a fresh directory or backup your .RData file.


extract_case_control.Rmd
========================

For context, see [485].

[485]: https://informatics.gpcnetwork.org/trac/Project/ticket/485
[bc_qa]: https://bitbucket.org/gpcnetwork/bc_qa

This program reads data from PCRONET CDM and matches it
with patient numbers from i2b2 to get a case-control
data set. It stores the data and some intermediate files in
an RData file.

Here's a summary of how the data was selected. We
identified four disease groups in Heron (KUMC's i2b2).

* Breast cancer (as identified in the SEER Site Summary),

* All other cancers (again from SEER),

* Diabetes (ICD9 250 or ICD10 E08-E13), and

* Ischemic heart disease (ICD9 410-414.99 or ICD10 I20-I25).

All males were excluded.

The first group represents cases and the other three
represent different control groups. Patients who qualified
for two or more of the above groups were excluded from the
analysis.

There were no date restrictions. In future versions, we may want
to consider some date restrictions and try matching subjects on
age.

We then merged that data with the PCORNET CDM to get 
any ICD9 codes associated with each patient.

Two companion programs, analyze_lasso.Rmd and analyze_sensitivity.Rmd,
take the resulting data set and produces simple graphs and analyses. 

There are several short text files that this program needs.

disease_group_databases.txt lists the information about databases
that store patient numbers for the cases (breast cancer) and the 
controls.

cdm_config.csv lists information needed to connect to the
cdm database.

icd9_labels.csv lists icd9 codes and labels.

i2b2_variables_databases.txt lists information on the various
i2b2 (or for KUMC, Heron) databases.

The program itself creates a small text file, timing_log.txt,
that helps me review the speed of various program chunks. It
is created using [knitr hooks http://yihui.name/knitr/hooks/].
Look for the timer function within knit_hooks$set.

This program was run on `r doc$date` using version `r doc$version`.
The original author is `r doc$author`. `r doc$maintainer`
is currently maintaining and enhancing this program
with the assistance of `r doc$assistants`.


```{r run_preliminaries, cache=FALSE}

# Backup image (just in case) and then start with a blank slate.

save.image("backup.RData")
rm(list=ls())

# Set qc option, if TRUE, print various intermediate values and quality checks.
# If drop_intermediate_objects is TRUE, then intermediate values are removed
# once they are no longer needed. You might keep the intermediate values
# around if you are debugging.

qc <- TRUE
drop_intermediate_objects <- TRUE

# Document when this program started.

start_time <- Sys.time()
if (qc) {
  cat("\n\nProgram started at ")
  print(start_time)
}

# clean out the old archive

if (exists("arc")) {rm(arc)}

# load required libraries.

library("ROracle")
library("RSQLite")
library("knitr")

# Check to see if you are in the proper subdirectory.

if (qc) {
  cat("\n\nQuality check: Are we in the correct directory?\n")
  print(getwd())
}

# Control wrapping

options(width=90)

# Here are some functions needed in this program.

source("create_utility_functions.R")

```

Set up storage for temporary data frames and matrices (d)
and notes for my own reference.

Design the appropriate sql queries.

```{r design_sql_queries, timer=TRUE}

# I need to put all my SQL queries in one spot so
# I can model one SQL query after the previous one.
# As I get better with SQL, I will move the queries
# closer to the program location where they get used.
#
#  Done!

sql <- NULL

```

This section reads in the patient ids for the various disease groups.

The file, disease_group_databases.txt, should have one entry for each file.
It also has information about which site the data 
comes from and which disease group the patients come from.

```{r get_patient_database_names, timer=TRUE}

di <- "/d1/home/ssimon/bc_qa/bc_case_control"
setwd(di)
fn <- "disease_group_databases.txt"
pa <- read.csv(file=fn, header=TRUE, stringsAsFactors=FALSE)

i_case <- pa$disease_group[pa$case==1]
i_control <- pa$disease_group[pa$case==0]

if (qc) print_random_rows(pa)
archive(pa,"patient file information")
```

Now loop across file names to get patient numbers for each group.

```{r get_patient_numbers_controls, timer=TRUE}

# pb: distinct patient numbers in each disease group

# Eventually, I will need to pull dates from this file. 
# Look for start_date in the observation_fact table.

sql[["distinct_patients"]] <-
  "select distinct patient_num
   from observation_fact"

pb <- NULL
i_control_codes <- which(pa$case==0)
for (i in i_control_codes) {
  p_connect <- dbConnect(SQLite(), dbname=pa$file_location[i])
  group_name <- pa$disease_group[i] 
  pb[[group_name]] <- dbGetQuery(p_connect, sql["distinct_patients"])
  dbDisconnect(p_connect)
}
archive(pb, "patient numbers for control groups")
```

Get cases separately because we need tumor_date.

```{r get_patient_numbers_cases, timer=TRUE}
sql[["start_date"]] <-
  "select *
   from observation_fact
   where concept_cd='SEER_SITE:26000'"

i_case_code <- which(pa$disease_group==i_case)[1] # [1] protects against duplicates.
p_connect <- dbConnect(SQLite(), dbname=pa$file_location[i_case_code])
px <- dbGetQuery(p_connect, sql["start_date"])
dbDisconnect(p_connect)
px$gp <- pa$disease_group[i_case_code]

# Let's look carefully at the cases.

o <- order(px$patient_num,px$start_date)
head(px[o, ])
dim(px)
dim(pb[[i_case]])

# There are a few duplicates, so I need to group by and select the minimum date.

px_count <- table(px$patient_num)
px_duplicates <- sample(names(px_count[px_count>1]), 5)
px[px$patient_num %in% px_duplicates, ]

table(substr(px$start_date, 1, 4)) # substr(1, 4) gives just the year.
table(substr(px$end_date, 1, 4)) 

# Neither download_date nor import_date are useful for analysis.

table(substr(px$download_date, 1, 4)) 
table(substr(px$import_date, 1, 4)) 

px <- px[!duplicated(px$patient_num), ]
dim(px)

px$tumor_date <- as.Date(substr(px$start_date, 1, 10), format="%Y-%m-%d")
cumulative_percent <- 100*(1:length(px$tumor_date)) / length(px$tumor_date)
plot(sort(px$tumor_date), cumulative_percent,
     axes=FALSE, type="l")
axis(side=2)
axis(side=1, at=as.Date(paste(10*(194:201), "01-01", sep="-")), labels=10*(194:201))
box()
archive(px, "patient numbers for cases")

if (qc) {
  for (i in 1:length(pb)) {
    print(names(pb)[i])
    print(dim(pb[[i]]))
    print_random_rows(pb[[i]])
  }
  print(dim(px))
  print_random_rows(px)
}
```

Combine everything.

```{r combine_everything, timer=TRUE}
pc <- px[ , c("gp", "patient_num", "tumor_date")]
for (k in 1:length(pb)) {
  pc <- rbind(pc, data.frame(gp=names(pb[k]), pb[[k]], tumor_date=NA, stringsAsFactors=FALSE))
}
archive(pc, "stacked patient numbers")
if (drop_intermediate_objects) rm(pb)
if (drop_intermediate_objects) rm(px)
if (qc) {
  print_random_rows(pc)
}
```

Next, you need to write the patient numbers to the same
location as the PCORnet CDM database.

The file cdm_config.csv contains login information.

```{r write_patient_numbers_to_cdm, timer=TRUE}

# This code borrowed from cdm_fun.Rmd.

cdm_config <- read.csv('../cdm_config.csv', stringsAsFactors=FALSE)
missing_config <- setdiff(c('account', 'password'), names(cdm_config))
stopifnot(length(missing_config) == 0)
c_connect <-
  dbConnect(Oracle(), cdm_config$account, cdm_config$password, cdm_config$access)

if (qc) {
  cat("Simple test")
  dbGetQuery(c_connect, "select * from pcornet_cdm.diagnosis where rownum < 10")
}

names(pc) <- toupper(names(pc))
dbWriteTable(c_connect, "PC", pc, overwrite=TRUE)

if (qc) dbListTables(c_connect)

```

Now pull out all the diagnosis codes associated with our patient list.

```{r extract_cdm_data, timer=TRUE}

# Note: some patients have no ICD9 codes. That may be okay.

sql[["matching_count"]] <- 
  "select distinct j.PATIENT_NUM, j.GP, p.PATID
   from pcornet_cdm.diagnosis P
   JOIN PC j
   on p.PATID = j.PATIENT_NUM
   where p.DX_TYPE = '09'"

if (qc) {
  mc <- dbGetQuery(c_connect, sql[["matching_count"]])
  count_unique_patients(mc, pc)
  print(table(mc$GP))
  print(table(pc$GP))
} 

sql[["patient_diagnoses"]] <-
  "select PATIENT_NUM, GP, DX, ADMIT_DATE as observation_date
   from pcornet_cdm.diagnosis
   join PC
   on PATID = PATIENT_NUM
   where DX_TYPE='09'"

ca <- dbGetQuery(c_connect, sql[["patient_diagnoses"]])
archive(ca, "Query from pcornet_cdm")
```


Now, let's allow ourselves the ability to screen out
any diagnosis codes that occur infrequently among the
cases.

```{r find_common_dx, timer=TRUE}

# Add a dx_count column that notes how often the DX appears
# among the case groups. This will allow you to select only
# the "popular" DX codes.

cb <- table(ca$DX[ca$GP==i_case])
archive(cb, "CDM diagnosis counts")

# Note: as.numeric is needed here because you need to strip out
# the names before merging.
# Also note that this merge statement will remove any diagnosis
# codes which occur only among the controls.

cc <- data.frame(DX=names(cb), dx_count=as.numeric(cb))
archive(cc, "CDM counts data.frame")

cd <- merge(ca, cc)
cd$dx_new <- align_icd9_codes(cd$DX)

if (qc) {
  print_random_rows(cc)
  print_random_rows(cd)
  print(length(unique(ca$DX)))
  print(length(unique(cd$DX)))
}
archive(cd, "Diagnoses merged with counts")
if (drop_intermediate_objects) rm(ca)
if (drop_intermediate_objects) rm(cb)
if (drop_intermediate_objects) rm(cc)

```

While you can get the nice labels for icd9 codes from i2b2, it is easier
just to pull them from another source. I chose labels from the
[https://www.cms.gov/medicare/coding/ICD9providerdiagnosticcodes/codes.html CMS]
site. 

```{r get_nice_names, timer=TRUE}

fn <- "icd9_labels.csv"
ce <- read.csv(fn, header=TRUE, as.is=TRUE, row.names=NULL)

ce$dx_new <- align_icd9_codes(ce$diagnosis_code)
ce$nice_label <- paste(strip_specials(ce$short_label),ce$DX,sep="_")
ce$dx_label <- paste("09", strip_specials(ce$short_label), ce$dx_new, sep="_")
archive(ce, "ICD9 codes and labels")

if (qc) print_random_rows(ce$nice_label)

# Here are diagnosis codes in cd, not found in ce.
if (qc) {
  print_random_rows(sort(setdiff(cd$DX,ce$DX)))
  print_random_rows(sort(setdiff(cd$dx_new,ce$dx_new)))
}

cf <- merge(cd, ce[, c("dx_new","dx_label")], all.x=TRUE, all.y=FALSE)
archive(cf, "CDM with nice labels")
if (drop_intermediate_objects) rm(cd)
if (drop_intermediate_objects) rm(ce)

if (qc) print_random_rows(cf)
```

Before we leave this section, save the DX and dx_label
values to allow later re-merges.

```{r allow_re_merges, timer=TRUE}
cg <- cf[!duplicated(cf$DX), c("DX", "dx_label")]
                               ```
if (qc) print_random_rows(cg)
archive(cg, "CDM code key")
```

In addition to CDM data, you can get lots of fun stuff
from i2b2 queries. The individual i2b2 queries appear
in i2b2_variables_databases.txt.

```{r find_i2b2_database_locations, timer=TRUE}

# I need to get dates from here. Look for start_time in
# the observation_fact table.
#
# Done!

fn <- "i2b2_variables_databases.txt"
ia <- read.csv(file=fn, header=TRUE, stringsAsFactors=FALSE)

archive(ia, "i2b2 file information")
if (qc) print(ia)

for (f in ia$file_location) {
  variables_conn <- dbConnect(SQLite(), dbname=f)
  dbWriteTable(variables_conn, "PC", pc, overwrite=TRUE)
  if (qc) dbListTables(variables_conn)
}
```

```{r read_i2b2, timer=TRUE}
of <- NULL

# Note where clause in the middle of this SQL code.
# I2B2 carriers along a lot of extra stuff when you
# export and this is an attempt to strip down to
# only the things you really need. 
#
# Unfortunately, it is specific to the particular
# i2b2 sources that you are using.

sql[["observation_fact"]] <-
  "select patient_num, GP, concept_cd, name_char, start_date AS observation_date 
   from observation_fact
   inner join (
     select concept_cd AS ccd, name_char
     from concept_dimension
     where
       concept_path LIKE '\\i2b2\\Procorders%' OR
       concept_path LIKE '\\i2b2\\Procedures%'
   )
   on concept_cd=ccd
   inner join (
     select PATIENT_NUM AS pn, GP
     from PC
   )
   on patient_num = pn"


for (i in 1:dim(ia)[1]) {
  f <- ia$file_location[i]
  v <- ia$variable_category[i]
  variables_conn <- dbConnect(SQLite(), dbname=f)
  of[[v]] <- dbGetQuery(variables_conn, sql[["observation_fact"]])
  of[[v]] <- cbind(of[[v]], abbreviation=ia$abbreviation[i])
}
```

Since we are importing multiple i2b2 files, we need to stack these
into a single data frame.

```{r stack_i2b2, timer=TRUE}
ib <- of[[1]]
for (i in 2:length(of)) {
  ib <- rbind(ib, of[[i]])
}

if (qc) {
  print(dim(ib))
  print_random_rows(ib)
  count_unique_patients(ib, pc)
}
archive(ib, "Stacked i2b2 data")
if (drop_intermediate_objects) rm(ia)
```

Now let's try to add counts.

```{r count_concepts}

# Add a concept_count column that notes how often concept_cd appears
# among the case groups. This will allow you to select only
# the "popular" concept_cd codes.

ic <- table(ib$concept_cd[ib$GP==i_case])

if (qc) print_random_rows(ic)
archive(ic, "Counts among cases")

# Note: as.numeric is needed here because you need to strip out
# the names before merging.

# Also note that this merge statement will remove any diagnosis
# codes which occur only among the controls.

id <- data.frame(concept_cd=names(ic), dx_count=as.numeric(ic))

if (qc) print_random_rows(id)
archive(id, "Convert to data.frame")
if (drop_intermediate_objects) rm(ic)

ie <- merge(ib, id)
ie$name_char <- strip_specials(ie$name_char)
ie$dx_label <- paste(ie$abbreviation, ie$name_char, ie$concept_cd, sep="_")

if (qc) print_random_rows(ie)
archive(ie, "Merge counts back in")
if (drop_intermediate_objects) rm(ib)
if (drop_intermediate_objects) rm(id)
```

Almost done. Put the cdm and i2b2 data into a single data frame.

```{r merge_cdm_and_i2b2, timer=TRUE}

i_names <- c("patient_num", "GP", "dx_count", "dx_label", "observation_date")
c_names <- c("PATIENT_NUM", "GP", "dx_count", "dx_label", "OBSERVATION_DATE")
iz <- ie[, i_names]
cz <- cf[, c_names]
if (drop_intermediate_objects) rm(cf)

names(iz) <- tolower(names(iz))
names(cz) <- tolower(names(cz))
                
lb <- rbind(cz, iz)
# make the labels look a bit nicer.
lb$dx_label <- tolower(lb$dx_label)
remove_list <-
  c("_icd9", "_kuh", "|proc_id", 
    "_at_", "_and_", "_for_",
    "_from_", 
    "_in_", "_into_", "_of_",
    "_or_", "_on_", "_to_", 
    "_use_", "_using_", "_with_") 
for (i in remove_list) {
  lb$dx_label <- sub(i, "_", lb$dx_label, fixed=TRUE)
}
lb$dx_label <- gsub(":", "_", lb$dx_label, fixed=TRUE)
lb$dx_label <- gsub("__", "_", lb$dx_label, fixed=TRUE)
lb$dx_label <- gsub("__", "_", lb$dx_label, fixed=TRUE)
```

Track counts.

```{r counts_in_cz_iz, timer=TRUE}

if (qc) {
  cat("How many patients in lb come from cz?\n")
  count_unique_patients(cz, lb)
  cat("How many patients in lb come from iz?\n")
  count_unique_patients(iz, lb)
  cat("How many patients are in lb versus pc?\n")
  count_unique_patients(lb, pc)
}
```

Below, we include only those codes that appear reasonably often
among the cases. This chunk should probably be moved back
into extract_case_conrol.Rmd

```{r include_only_popular_icd9_codes, timer=TRUE}
cutoff <- 100
n1 <- length(unique(lb$dx_label))
lc <- lb[lb$dx_count >= cutoff, ]
rm(lb)
n2 <- length(unique(lc$dx_label))
archive(lc, "popular codes only")
```

The initial data set has `r n1` unique ICD9 codes. After eliminating
the codes that occur less than `r cutoff` times among the cases,
there are `r n2` unique ICD9 codes left.

Optionally, create training and test data sets. Again, this chunk should
probably be moved back into extract_case_control.Rmd.

```{r create_training_and_test_data_sets}
proportion_training <- 0.7
unique_patients <- sort(unique(lc[, "patient_num"]))
select <- sample(c("Train","Test"), length(unique_patients), replace=TRUE,
                 prob=c(proportion_training, 1-proportion_training))
if (qc) {print_random_rows(select)}
ld <- merge(lc, data.frame(patient_num=unique_patients, select=select))
archive(ld, "test and training sets")
le <- ld[ld$select=="Train", ]
archive(le, "training set only")
lf <- ld[ld$select=="Test", ]
archive(lf, "test set only")
```

Before we go, save the concept_cd and dx_label values
so that we can re-merge some files later.

```{r save_for_re_merges, timer=TRUE}
ig <- ie[!duplicated(ie$concept_cd), c("concept_cd", "dx_label")]
archive(ig, "I2B2 code key")
if (drop_intermediate_objects) rm(ie)
if (qc) print_random_rows(ig)
names(cg) <- c("original_code", "dx_label")
names(ig) <- c("original_code", "dx_label")
zg <- rbind(cg, ig)
archive(zg, "Combined code key")
write.csv(zg, file="code_key.csv", row.names=FALSE)
if (qc) print_random_rows(zg)

```

Now save everything for later use.

The file case_control_data.RData stores only the important stuff.

```{r save_important_data, timer=TRUE}
save(
  lc, le, lf, pc, i_case, i_control,
  file="case_control_data.RData")
```

The file extract_case_control.RData stores everything, including intermediate files.

```{r save_everything, timer=TRUE}
# case_control_data.RData stores only the important stuff.

save.image(file="extract_case_control.RData")
```

Well done. Here is how long everything took.

```{r display_timing_log}
if (qc) {
  cat("Program began at ")
  cat(as.character(start_time))
  cat("\nProgram ended at ")
  cat(as.character(Sys.time()))
  cat("\n\n")
  tm <- read.table("timing_log.txt")$V1
  cat(paste(tm, collapse="\n"))
}
```
