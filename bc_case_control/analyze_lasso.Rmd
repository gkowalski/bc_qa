```{r create_basic_documentation, echo=TRUE}
# Before anything else, set global options
opts_chunk$set(echo=TRUE, cache=TRUE, error=TRUE)

doc <- NULL
doc$run.date <- date()
doc$version <- system(' git rev-parse HEAD', intern=TRUE)
doc$author <- "Steve Simon (KUMC)"
doc$maintainer <- "Steve Simon (KUMC)"
doc$assistants <- "Dan Connolly"
```


Case-control
============
For context, see [485].

[485]: https://informatics.gpcnetwork.org/trac/Project/ticket/485
[bc_qa]: https://bitbucket.org/gpcnetwork/bc_qa

This program analyzes a case-control data set. It produces some simple
tables and graphs based on the lasso and/or elastic net models.

A summary of how the data were originally collected appears in 
extract_case_control.Rmd.

A companion program, extract_case_control, produces the data set
used by this program. The data files from this program are used
by display_lasso.Rmd.

This program was run on `r doc$date` using version `r doc$version`.
The original author is `r doc$author`. `r doc$maintainer`
is currently maintaining and enhancing this program
with the assistance of `r doc$assistants`.


```{r load_required_libraries, echo=TRUE}
save.image("backup.RData")
rm(list=ls())

# Don't wrap so much
options(width=90)

# load the required libraries
library("reshape2")
library("glmnet")
library("knitr")
library("Matrix")

# Load the case-control data sets
load("case_control_data.RData")
# The next two lines are a temporary transition.
i_control <- i.control
i_case <- i.case

# Document when this program started.

verbose <- TRUE
start_time <- Sys.time()
if (verbose) {
  cat("\n\nProgram started at ")
  print(start_time)
  cat("\n\nQuality check: Are we in the correct directory? ")
  print(getwd())
}

# Set up timer

knit_hooks$set(timer = 
  function(before, options, envir) {
    if (before) {
      current_time <<- Sys.time()
      m <- paste("Chunk", options$label, "started at", as.character(current_time), ".  \n")
      print(m)
      return(m)
    } else {
      elapsed_time <- Sys.time() - current_time
      elapsed_message <- paste(round(elapsed_time, 1), attr(elapsed_time, "units"))
      m <- paste("Chunk", options$label, "used", elapsed_message)
      print(m)
      if (exists("timing_log")) {
        timing_log <<- c(timing_log, m)
      } else {
          timing_log <<- m
      }
    write.table(timing_log, file="timing_log.txt", row.names=FALSE, col.names=FALSE)  
    return(m)
    }
  }
) 


```

Below, we include only those codes that appear reasonably often
among the cases.

```{r include_only_popular_icd9_codes, timer=TRUE}
cutoff <- 5
n1 <- length(unique(lb$dx_label))
lc <- lb[lb$dx_count >= cutoff, ]
n2 <- length(unique(lc$dx_label))
```

The initial data set has `r n1` unique ICD9 codes. After eliminating
the codes that occur less than `r cutoff` times among the cases,
there are `r n2` unique ICD9 codes left.

```{r experiment_with_sparse_matrices, timer=TRUE}
# This is a work in progress. If the number of variables gets
# very large, you may need to switch from the standard
# matrix storage to sparse matrices.
create_sparse_matrix <- function(tst) {
  irow <- factor(tst$patient_num)[!is.na(tst$dx_label)]
  jcol <- factor(tst$dx_label)[!is.na(tst$dx_label)]
  o <- order(tst$patient_num)
  pg <- tst[o, c("patient_num", "gp")]
  pg <- pg[!duplicated(pg$patient_num), ]
  row_names <- sort(as.character(unique(irow)))
  column_names <- sort(as.character(unique(jcol)))
  sm <- sparseMatrix(i=as.numeric(irow), j=as.numeric(jcol), x=1)
  dimnames(sm) <- list(row_names, column_names)
  return(list(pg=pg, sm=sm))
}

create_traditional_matrix <- function(tst) {
  tm <- dcast(tst ,patient_num + gp ~ dx_label,length)
  pg <- tm[, 1:2]
  return(list(pg=pg, tm=tm[, -(1:2)]))
}

quality_check <- FALSE

# The time to run a quality check is prohibitive when the number of
# columns in the matrix is in the thousands.

if (quality_check) {
  print(Sys.time())
  tst1 <- create_sparse_matrix(lc)
  print(Sys.time())
  tst2 <- create_traditional_matrix(lc)
  print(Sys.time())
  print(format(object.size(tst1), "auto"))
  print(format(object.size(tst2), "auto"))
  sample_list <- sample(dimnames(tst1$sm)[[2]],5)
  for (v in sample_list) {
    print(v)
    print(table(tst1$pg$gp,tst1$sm[, v]))
    print(table(tst2$gp, tst2[, v]))
  }
}
```

Now, run a lasso/elastic net model for each control group.

```{r store_lasso_models, timer=TRUE}
all_models <- NULL
all_cv <- NULL
for (ic in i_control) {
  pc1 <- create_sparse_matrix(lc[lc$gp %in% c(i_case, ic), ])
  iv_matrix <- pc1$sm
  dv_vector <- as.numeric(pc1$pg[,"gp"]==i_case)
  en <- glmnet(
    x=iv_matrix, alpha=0.5, 
    standardize=FALSE, lower.limits=0, 
    y=dv_vector,
    family="binomial")
  all_models[[ic]] <- en
  cv <- cv.glmnet(
    x=iv_matrix, alpha=0.5,
    standardize=FALSE, lower.limits=0, 
    y=dv_vector,
    lambda=en$lambda,
    family="binomial",
    type.measure="class")
  all_cv[[ic]] <- cv
}
```

Store everything for later use.

```{r save-everything, timer=TRUE}
save.image(file="analyze_lasso.RData")
```

Well done. Here is how long everything took.

```{r display_timing_log}
if (verbose) {
  cat("Program began at ")
  cat(as.character(start_time))
  cat("\nProgram ended at ")
  cat(as.character(Sys.time()))
  cat("\n\n")
  tm <- read.table("timing_log.txt")$V1
  cat(paste(tm, collapse="\n"))
}
```
