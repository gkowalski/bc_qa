```{r create_basic_documentation, echo=TRUE}
# Before anything else, set global options
opts_chunk$set(echo=TRUE, cache=TRUE, error=TRUE)

doc <- NULL
doc$run.date <- date()
doc$version <- system(' git rev-parse HEAD', intern=TRUE)
doc$author <- "Steve Simon (KUMC)"
doc$maintainer <- "Steve Simon (KUMC)"
doc$assistants <- "Dan Connolly"
```


Case-control
============
For context, see [485].

[485]: https://informatics.gpcnetwork.org/trac/Project/ticket/485
[bc_qa]: https://bitbucket.org/gpcnetwork/bc_qa

This program takes a model and evaluates it on patients.It stores the data and some
intermediate files in an RData file.


Currently, this program takes the model developed by display_lasso.Rmd,
but it can be adapted for models from other programs without too much
difficulty.

Here's a rough outline of how the program works.

1. Get a list of all the variables included in the model. 
2. Get a list of patients.
3. Find any records for these patients that have variables in the model.
4. Calculate a prediction based on those variables.
5. Assess the quality of that prediction.

This program repeats a lot of work done in extract_case_control, but it 
saves some time by only extracting records needed for the model.

This program was run on `r doc$date` using version `r doc$version`.
The original author is `r doc$author`. `r doc$maintainer`
is currently maintaining and enhancing this program
with the assistance of `r doc$assistants`.


```{r run_preliminaries}

# Backup image (just in case) and then start with a blank slate.

save.image("backup.RData")
rm(list=ls())

# Set verbose option, if TRUE, print various intermediate values and quality checks.

verbose <- TRUE

# Document when this program started.

start_time <- Sys.time()
if (verbose) {
  cat("\n\nProgram started at ")
  print(start_time)
}

# load required libraries.

library("ROracle")
library("RSQLite")
library("knitr")

# Check to see if you are in the proper subdirectory.

if (verbose) {
  cat("\n\nQuality check: Are we in the correct directory?\n")
  print(getwd())
}

# Control wrapping

options(width=90)

# Here are some functions needed in this program.

source("create_utility_functions.R")

```

Set up an archive for intermediate data files.

```{r setup_archive_storage, timer=TRUE}

# This program uses many different data frames. Data frames associated
# with different databases will typically start with the same letter.
#   p: individual patient ids for each disease group
#   i: data from additional i2b2 queries
#   c: data from the CDM
#
# Most of these are intermediate data frames.
# As a quality check and to test new code, I will store the intermediate
# data frames in a list called archive

archive <- NULL
notes <- NULL    # temporary--remove when transitioned to archive
archive$header <- "This is a list of various intermediate files."

```

Read the model. Get code_key to link to original names.

```{r read_model}
ma <- read.csv("code_key.csv", as.is=TRUE)
load(file="display_lasso.RData")
mb <- or_consensus[or_consensus$di==3, c("lb", "co")]
mc <- merge(x=ma, y=mb, by.x="dx_label", by.y="lb", all=FALSE)
if (verbose) {
  print_random_rows(mc)
}
```

Design the appropriate sql queries.

```{r design_sql_queries, timer=TRUE}

# I need to put all my SQL queries in one spot so
# I can model one SQL query after the previous one.
# As I get better with SQL, I will move the queries
# closer to the program location where they get used.

sql <- NULL

sql[["pcornet_subset"]] <-
  "select distinct j.PATIENT_NUM, j.GP, p.DX
   from pcornet_cdm.diagnosis P
   join PC j
   on p.PATID = j.PATIENT_NUM
   where p.DX_TYPE='09'
   join MC m
   on p.DX=m.original_code"

sql[["heron_subset"]] <-
  "select o.patient_num, o.concept_cd, c.name_char, o.start_time 
   from observation_fact o
   inner join (concept_dimension c
     where concept_path LIKE '\\i2b2\\Procorders%' OR
       concept_path LIKE '\\i2b2\\Procedures%')
   ON o.concept_cd=c.concept_cd
   join MC m
   on o.concept_cd=m.original_code"

if (verbose) {
  for (i in 1:length(sql)) {
    print(names(sql)[i])
    cat(sql[[i]])
    cat("\n\n")
  }
}

```

This section reads in the patient ids for the various disease groups.

The file, disease_group_databases.txt, should have one entry for each file.
It also has information about which site the data 
comes from and which disease group the patients come from.

```{r get_patient_database_names, timer=TRUE}

di <- "/d1/home/ssimon/bc_qa/bc_case_control"
setwd(di)
fn <- "disease_group_databases.txt"
pa <- read.csv(file=fn, header=TRUE, stringsAsFactors=FALSE)
archive$pa_patient_identifiers_file_information <- pa

if (verbose) print(pa)

i_case <- pa$disease_group[pa$case==1]
i_control <- pa$disease_group[pa$case==0]

```

Now get patient_numbers for the cases.

```{r get_patient_numbers, timer=TRUE}

# pb: distinct patient numbers in each disease group

# Eventually, I will need to pull dates from this file. 
# Look for start_date in the observation_fact table.
pb <- NULL
i.list <- which(pa$case==1)
for (i in i.list) {
  p_connect <- dbConnect(SQLite(), dbname=pa$file_location[i])
  group_name <- pa$disease_group[i] 
  pb[[group_name]] <- dbGetQuery(p_connect, sql["distinct_patients"])
  dbDisconnect(p_connect)
  archive_name <- paste("pb", group_name, "patient numbers", sep="_")
  archive[[archive_name]] <- pb[[group_name]]
}

if (verbose) {
  for (i in 1:length(pb)) {
    print(names(pb)[i])
    print(dim(pb[[i]]))
    print_random_rows(pb[[i]])
  }
}

pc <- data.frame(gp=names(pb[1]), pb[[1]], stringsAsFactors=FALSE)
archive$pc_stacked_patient_numbers <- pc

if (verbose) {
  print_random_rows(pc)
}
```

Next, you need to write the patient numbers and variable names
to the same location as the PCORnet CDM database.

```{r write_patient_numbers_to_cdm, timer=TRUE}

# This code borrowed from cdm_fun.Rmd.

cdm_config <- read.csv('../cdm_config.csv', stringsAsFactors=FALSE)
missing_config <- setdiff(c('account', 'password'), names(cdm_config))
stopifnot(length(missing_config) == 0)
c_connect <-
  dbConnect(Oracle(), cdm_config$account, cdm_config$password, cdm_config$access)

if (verbose) {
  cat("Simple test")
  dbGetQuery(c_connect, "select * from pcornet_cdm.diagnosis where rownum < 10")
}

names(pc) <- toupper(names(pc))
dbWriteTable(c_connect, "PC", pc, overwrite=TRUE)

if (verbose) dbListTables(c_connect)

```

Now pull out all the diagnosis codes associated with our patient list.

```{r extract_cdm_data, timer=TRUE}

# Note: there are patients who are in the various
# disease groups, but who do not have any ICD9 codes.
# I need to investigate why.

if (verbose) {
  mc <- dbGetQuery(c_connect, sql[["matching_count"]])
  count_unique_patients(mc, pc)
  print(table(mc$GP))
  print(table(pc$GP))
} 

ca <- dbGetQuery(c_connect, sql[["patient_diagnoses"]])
archive$ca_SQL_query_from_pcornet_cdm <- ca
```


Now, let's allow ourselves the ability to screen out
any diagnosis codes that occur infrequently among the
cases.

```{r find_common_dx, timer=TRUE}

# Add a dx_count column that notes how often the DX appears
# among the case groups. This will allow you to select only
# the "popular" DX codes.

cb <- table(ca$DX[ca$GP==i_case])
archive$cb_cdm_diagnosis_counts_table <- cb

# Note: as.numeric is needed here because you need to strip out
# the names before merging.
# Also note that this merge statement will remove any diagnosis
# codes which occur only among the controls.

cc <- data.frame(DX=names(cb), dx_count=as.numeric(cb))
archive$cc_cdm_diagnosis_counts_data_frame <- cc

cd <- merge(ca, cc)
archive$cd_cdm_diagnoses_merged_with_counts <- cd

if (verbose) {
  print_random_rows(cc)
  print_random_rows(cd)
  print(length(unique(ca$DX)))
  print(length(unique(cd$DX)))
}

```

While you can get the nice labels for icd9 codes from i2b2, it is easier
just to pull them from another source. I chose labels from the
[https://www.cms.gov/medicare/coding/ICD9providerdiagnosticcodes/codes.html CMS]
site. 

```{r get_nice_names, timer=TRUE}

fn <- "icd9_labels.csv"
ce <- read.csv(fn, header=TRUE, as.is=TRUE, row.names=NULL)
archive$ce_icd9_codes_and_labels <- ce

ce$nice_label <- paste(strip_specials(ce$short_label),ce$DX,sep="_")
if (verbose) print_random_rows(ce$nice_label)


cd$dx_new <- align_icd9_codes(cd$DX)
ce$dx_new <- align_icd9_codes(ce$diagnosis_code)

# Here are diagnosis codes in cd, not found in ce.
if (verbose) {
  print_random_rows(sort(setdiff(cd$DX,ce$DX)))
  print_random_rows(sort(setdiff(cd$dx_new,ce$dx_new)))
}

ce$dx_label <- paste(strip_specials(ce$short_label), ce$dx_new, sep="_")
cf <- merge(cd, ce[, c("dx_new","dx_label")], all.x=TRUE, all.y=FALSE)
archive$cf_dx_codes_and_nice_labels <- cf

if (verbose) print_random_rows(cf)

```

In addition to CDM data, you can get lots of fun stuff
from i2b2 queries.

Two that I have so far are imaging variables, and procedure codes.

```{r now_get_i2b2_codes, timer=TRUE}

# I need to get dates from here. Look for start_time in
# the observation_fact table.

fn <- "i2b2_variables_databases.txt"
ia <- read.csv(file=fn, header=TRUE, stringsAsFactors=FALSE)
archive$ia_i2b2_file_information <- ia
if (verbose) print(ia)

for (f in ia$file_location) {
  variables_conn <- dbConnect(SQLite(), dbname=f)
  dbWriteTable(variables_conn, "PC", pc, overwrite=TRUE)
  if (verbose) dbListTables(variables_conn)
}

of <- NULL

sql[["observation_fact"]] <-
  "select distinct patient_num, GP, concept_cd, name_char 
   from observation_fact
   inner join (
     select concept_cd AS ccd, name_char
     from concept_dimension
     where
       concept_path LIKE '\\i2b2\\Procorders%' OR
       concept_path LIKE '\\i2b2\\Procedures%'
   )
   on concept_cd=ccd
   inner join (
     select PATIENT_NUM AS pn, GP
     from PC
   )
   on patient_num = pn"

for (f in ia$file_location) {
  variables_conn <- dbConnect(SQLite(), dbname=f)
  of[[f]] <- dbGetQuery(variables_conn, sql[["observation_fact"]])
}

# Since we are importing multiple i2b2 files, we need to stack these
# into a single data frame.

ib <- of[[1]]
for (i in 2:length(of)) {
  ib <- rbind(ib, of[[i]])
}

archive$ib_stacked_data <- ib
if (verbose) {
  print(dim(ib))
  print_random_rows(ib)
  count_unique_patients(ib, pc)
}

# Add a concept_count column that notes how often concept_cd appears
# among the case groups. This will allow you to select only
# the "popular" concept_cd codes.

ic <- table(ib$concept_cd[ib$GP==i_case])
archive$ic_frequency_counts_among_the_cases <- ic

# Note: as.numeric is needed here because you need to strip out
# the names before merging.

# Also note that this merge statement will remove any diagnosis
# codes which occur only among the controls.

id <- data.frame(concept_cd=names(ic), dx_count=as.numeric(ic))
archive$id_convert_table_to_data_frame <- id
if (verbose) print_random_rows(id)

ie <- merge(ib, id)
ie$name_char <- strip_specials(ie$name_char)
ie$dx_label <- paste(ie$name_char, ie$concept_cd, sep="_")
archive$ie_merge_frequency_counts <- ie
if (verbose) print_random_rows(ie)

i_names <- c("patient_num", "GP", "dx_count", "dx_label")
c_names <- c("PATIENT_NUM", "GP", "dx_count", "dx_label")
iz <- ie[, i_names]
cz <- cf[, c_names]

names(iz) <- tolower(names(iz))
names(cz) <- tolower(names(cz))
                
lb <- rbind(cz, iz)
st <- pc

if (verbose) {
  cat("How many patients in lb come from cz?\n")
  count_unique_patients(cz, lb)
  cat("How many patients in lb come from iz?\n")
  count_unique_patients(iz, lb)
  cat("How many patients are in lb versus pc?\n")
  count_unique_patients(lb, pc)
}

```

Now save everything for later use.

```{r save-everything, timer=TRUE}
# extact_case_control.RData stores everything, including intermediate files.
# case_control_data.RData stores only the important stuff.

save.image(file="extract_case_control.RData")
save(
  lb, st, i_case, i_control,
  file="case_control_data.RData")
```

Well done. Here is how long everything took.

```{r display_timing_log}
if (verbose) {
  cat("Program began at ")
  cat(as.character(start_time))
  cat("\nProgram ended at ")
  cat(as.character(Sys.time()))
  cat("\n\n")
  tm <- read.table("timing_log.txt")$V1
  cat(paste(tm, collapse="\n"))
}
```
